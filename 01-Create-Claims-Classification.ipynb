{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "636bf905-4d45-4fdf-8ee7-41493ea5751a",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022bfec1-9efb-47f1-beb8-586d1d0ccc5a",
   "metadata": {},
   "source": [
    "In this notebook we will learn how to **characterize bus repairs** from free text descriptions, entered by users. This will be accomplished through the use of **Natural Language Processing (NLP)**.\n",
    "\n",
    "This will allow you to discover, step by step, how you can create the code doing the repair text processing.  In the last part of the workshop, this code will be **packaged to create a service** that you can query from an application.\n",
    "\n",
    "We will be training the model on simulated data. (Feel free to explore notebook [00-Generate-Sample-Claims-Data.ipynb](00-Generate-Sample-Claims-Data.ipynb) to see how this data was simulated.)  Once our model is trained, we can test the model by entering a bus repair issue (e.g. the brakes feel soft when I press on them) and check if the model has correctly categorised the claim.  Repairs will be categorized as:  Brakes, Starter or Other.\n",
    "\n",
    "Ready? Let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cba8fe-4b7d-453b-9cf4-4fe485feccca",
   "metadata": {},
   "source": [
    "# Environment initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cadbffe-164d-40e7-a66e-36c5e0cd4718",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "\n",
    "When we Launched JupyterLab, we selected the `Tensorflow` notebook image. This already has some key libraries installed for us, but we have also added all the libraries (and their versions) which we are relying on into the `nb-requirements.txt` file. This will make it easier for us to repeatably run this code and share it with others. \n",
    "\n",
    "We install these now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e5e797-af85-47fe-80f4-29f67107ad10",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r nb-requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cd4352-a1a1-4c9e-b1f3-7658186c591b",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Now that our libraries are installed, we need to import them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee87a88b-e1bc-4bf9-b756-818c8bb69e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48f6f8e-6f58-4d47-b818-c6983e9e73fc",
   "metadata": {},
   "source": [
    "## Create Training and Testing data sets\n",
    "\n",
    "Now that we have loaded the libraries we need, the first step in our journey is to be able to take our raw data and divide it into testing and training sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693d83e0-8968-4ad8-9fea-56d881637b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#============================================================================\n",
    "#Determine what the training and testing percentages, of the data set, will be.\n",
    "#============================================================================\n",
    "training_portion = .80         # Use 80% of data for training, 20% for testing\n",
    "max_words        = 1000        # Max words in text input\n",
    "\n",
    "data             = pd.read_csv('dataset/testdata1.csv')\n",
    "\n",
    "#============================================================================\n",
    "# TRY:  uncomment the below print statement and print out first 5 rows of \n",
    "# generated claims so you can see what data looks like.\n",
    "#\n",
    "# print(data.head())\n",
    "#============================================================================\n",
    "\n",
    "train_size       = int(len(data) * training_portion)\n",
    "\n",
    "#============================================================================\n",
    "# FUNCTION:  train_test_split\n",
    "# This function splits the data into training and test sets.  \n",
    "# Inputs:   raw data and determined train_size.\n",
    "#============================================================================\n",
    "def train_test_split(data, train_size):\n",
    "    train        = data[:train_size]\n",
    "    test         = data[train_size:]\n",
    "    return train, test\n",
    "\n",
    "train_cat, test_cat   = train_test_split(data.iloc[:,1], train_size)  # label data is second column\n",
    "train_text, test_text = train_test_split(data.iloc[:,0], train_size)  # text data is first column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e0af16-5205-4479-84f8-8dce26e328b0",
   "metadata": {},
   "source": [
    "## Tokenize the Data sets\n",
    "\n",
    "After we have training and testing sets, we need to **tokenize the data**.  This means that we convert text documents into contextual vectors which contain numeric representations (index of where those words occur in a word dictionary) of the words in the documents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d01f6a-4e86-4ce6-bc37-b31b7747a877",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize              = Tokenizer(num_words=max_words, char_level=False)\n",
    "tokenize.fit_on_texts(train_text) # fit tokenizer to our training text data\n",
    "\n",
    "#============================================================================\n",
    "#x_train and x_test are the vectorization of the text data (which is a claim)\n",
    "#============================================================================\n",
    "x_train               = tokenize.texts_to_matrix(train_text)\n",
    "x_test                = tokenize.texts_to_matrix(test_text)\n",
    "\n",
    "#============================================================================\n",
    "# TRY:  uncomment the below print statement and observe the rows in the \n",
    "# newly created matrix.\n",
    "#\n",
    "# print(x_train)\n",
    "# ===========================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9cd957-445a-4a28-a6e6-3c801304e277",
   "metadata": {},
   "source": [
    "Before we can make a prediction, we will need to pass any future data through the tokenizer to transform it into feature vectors. We now save the tokenizer so we can use it later: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2234b1a5-7f0b-40bd-86d8-3cad5d2a6747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0144e8-413e-42ae-9ef0-1e55a7dfad46",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenize, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d4dcc9-c760-465d-9d18-bd274f66c7d3",
   "metadata": {},
   "source": [
    "## Using Scikit-learn\n",
    "We will be using the Label Encoder utility from Scikit-learn to convert label strings to numbered index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a64ce63-9c28-410d-b1c9-251a61b0c3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#============================================================================\n",
    "# Convert label strings to numbered index\n",
    "#============================================================================\n",
    "encoder              = LabelEncoder()  \n",
    "encoder.fit(train_cat)\n",
    "y_train              = encoder.transform(train_cat)\n",
    "y_test               = encoder.transform(test_cat)\n",
    "\n",
    "#============================================================================\n",
    "# Note: for each row in the data, each entry represents the value of the label\n",
    "# Example:  [2 1 1 2 1 1 0 ...  which corresponds to starter, other,\n",
    "# other, starter, other, other, brakes ...\n",
    "#\n",
    "#============================================================================\n",
    "# TRY:  uncomment the below print statement.  What would you expect y_train\n",
    "# to look like?  \n",
    "#\n",
    "# print(y_train)\n",
    "#============================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcf16bf-c977-4900-8aa0-e45a719141e2",
   "metadata": {},
   "source": [
    "## One Hot Encoding\n",
    "\n",
    "We need to create labels (categories such as Brakes or Starter) for our test data, convert the labels to numbered index and then use one-hot encoding.\n",
    "\n",
    "**One hot encoding** allows the representation of categorical data to be more expressive. Many machine learning algorithms cannot work with categorical data directly. **The categories must be converted into numbers**. This is required for both input and output variables that are categorical.\n",
    "\n",
    "After we have converted the labels using one-hot encoding, we will be ready to build our main NLP model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26517820-122d-41e3-97f1-ef3b4a09289e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#============================================================================\n",
    "# Convert the labels to a one-hot representation\n",
    "#\n",
    "# One Hot Encoding replaces the column of labels whose (values are 0 or 1 or 2)\n",
    "# with 3 columns each representing 1 label value.  For example, the label \n",
    "# 'other' is replaced by the vector 0 1 0, the label 'starter' is replaced by\n",
    "# the vector 0 0 1, the label 'brakes' is replaced by the vector 1 0 0\n",
    "#============================================================================\n",
    "num_classes          = len(set(y_train))  # set() creates a unique set of objects\n",
    "y_train              = to_categorical(y_train, num_classes)  \n",
    "y_test               = to_categorical(y_test, num_classes)\n",
    "\n",
    "#============================================================================\n",
    "# TRY:  uncomment the below print statements in order to inspect the \n",
    "# dimenstions of our training and test data.\n",
    "# y_train may appear as y_train shape: (159, 3) which represents 159 rows, 3 cols\n",
    "#\n",
    "#print('x_train shape:', x_train.shape)\n",
    "#print('x_test shape:', x_test.shape)\n",
    "#print('y_train shape:', y_train.shape)\n",
    "#print('y_test shape:', y_test.shape)\n",
    "#============================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a6416d-6cd6-49d8-9aca-2379b0702a43",
   "metadata": {},
   "source": [
    "\n",
    "## Building the model\n",
    "\n",
    "Once the model is trained, we can test our model by entering a repair issue (e.g. the brakes feel soft when I press on them) and check if the model has correctly characterized the repair issue. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc05d7a6-7e54-4208-b743-0ffbb866dec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#============================================================================\n",
    "# Build model\n",
    "#============================================================================\n",
    "layers               = keras.layers\n",
    "models               = keras.models\n",
    "model                = models.Sequential()\n",
    "model.add(layers.Dense(512, input_shape=(max_words,), activation='relu'))  # Hidden layer with 512 nodes\n",
    "model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "#============================================================================\n",
    "# relu, softmax, categorical_crossentropy are telling the model how to do some \n",
    "# internal calculations.  Softmax is telling the model to calculate \n",
    "# probabilities for each category in each document.  If you only had yes, \n",
    "# or no you would use sigmoid instead of softmax.\n",
    "#============================================================================\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#============================================================================\n",
    "# VARIABLES\n",
    "# history    - normally used to plot learning curves.  \n",
    "# fit        - calculates the weights in the model. \n",
    "# batch_size - tells the internal calculations how many rows to process at 1 time\n",
    "# epochs     - num of times model calculations will pass through the entire data\n",
    "#============================================================================\n",
    "batch_size          = 32\n",
    "epochs              = 2\n",
    "history = model.fit(x_train, y_train,      \n",
    "                    batch_size=batch_size,  \n",
    "                    epochs=epochs,         \n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)\n",
    "\n",
    "#============================================================================\n",
    "# evaluate func compares the model predictions with the actual known test values\n",
    "#============================================================================\n",
    "score = model.evaluate(x_test, y_test,       \n",
    "                       batch_size=batch_size, verbose=1)\n",
    "\n",
    "#============================================================================\n",
    "# TRY:  uncomment the below print statements to see the test loss and accuracy\n",
    "# of our model\n",
    "#\n",
    "#print('Test loss:', score[0])\n",
    "#print('Test accuracy:', score[1])\n",
    "#============================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7caa08b-cfca-41ad-9a11-281aad8e67e8",
   "metadata": {},
   "source": [
    "We can save our trained model, and our encoder classes, both of which we will need to use later when we create a model service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441b36cd-97f4-4ecc-a162-642cf66ef0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/repairmodel.h5')\n",
    "\n",
    "text_labels = encoder.classes_   #ndarray of output values (labels or classes)  e.g. other, brakes, starter\n",
    "\n",
    "with open('text_labels.npy', 'wb') as f:\n",
    "    np.save(f, text_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cecb48-13d4-4387-ac8f-37bfc797c8f4",
   "metadata": {},
   "source": [
    "\n",
    "## Let's test our model!\n",
    "\n",
    "Now that we have a model, we would like to generate a prediction (e.g. categorize the repair issue as:  Brakes, Starter or Other). We must create a prediction function which takes in a string and returns the predicted catagory of the error:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157b9c35-7cf9-4302-b489-64fb2df23e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(single_test_text):\n",
    "\n",
    "    text_as_series = pd.Series(single_test_text) #do a data conversion\n",
    "    single_x_test = tokenize.texts_to_matrix(text_as_series)\n",
    "    single_prediction = model.predict(np.array([single_x_test]))\n",
    "\n",
    "    single_predicted_label = text_labels[np.argmax(single_prediction)]\n",
    "    \n",
    "    return {'prediction': single_predicted_label}\n",
    "\n",
    "#========================================\n",
    "#Run the firs time in order to save the model\n",
    "#=========================================\n",
    "single_test_text = 'turn the key and nothing happens' \n",
    "print(single_test_text)    #print out the repair being categorized\n",
    "\n",
    "prediction = predict(single_test_text) \n",
    "print(prediction)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e603b3-f4e2-428f-857b-8829d4ba270f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==========================================================================\n",
    "# TRY: uncomment the below to test the predict function.  We will test 3\n",
    "# repair cases so that we can see if the model can properly categorize\n",
    "# a Brake, a Starter and an Other repair text\n",
    "\n",
    "single_test_text = 'press brake pedal and car wont stop'\n",
    "#single_test_text = 'turn key over and hear a clicking sound' \n",
    "#single_test_text = 'there is fluid leaking from the engine' \n",
    "\n",
    "print(single_test_text)    #print out the repair being categorized\n",
    "model = keras.models.load_model('models/repairmodel.h5')  #load the model before called predict function\n",
    "prediction = predict(single_test_text) \n",
    "print(prediction)                           #print the predicti\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
