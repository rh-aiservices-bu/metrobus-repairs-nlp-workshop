{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "636bf905-4d45-4fdf-8ee7-41493ea5751a",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022bfec1-9efb-47f1-beb8-586d1d0ccc5a",
   "metadata": {},
   "source": [
    "In this notebook we will learn how to **characterize bus repairs** from free text descriptions, entered by users. This will be accomplished through the use of **Natural Language Processing (NLP)**.\n",
    "\n",
    "This will allow you to discover, step by step, how you can create the code doing the repair text processing.  In the last part of the workshop, this code will be **packaged to create a service** that you can query from an application.\n",
    "\n",
    "We will be training the model.  Once our model is trained, we can test the model by entering a bus repair issue (e.g. the brakes feel soft when I press on them) and check if the model has correctly characterized the claim.  Repairs will be categorized as:  Brakes, Starter or Other.\n",
    "\n",
    "Ready? Let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cba8fe-4b7d-453b-9cf4-4fe485feccca",
   "metadata": {},
   "source": [
    "# Environment initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cadbffe-164d-40e7-a66e-36c5e0cd4718",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "First, we'll need to **install some libraries** that are not part of our container image. Normally, **Red Hat OpenShift Data Science** is already taking care of this for you, based on what it detects in the code. **Red Hat OpenShift Data Science** will reinstall all those libraries for you every time you launch the notebook!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cd4352-a1a1-4c9e-b1f3-7658186c591b",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Of course, we'll need to import various packages. They are either built in the notebook image you are running, or have been installed in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee87a88b-e1bc-4bf9-b756-818c8bb69e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow import keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48f6f8e-6f58-4d47-b818-c6983e9e73fc",
   "metadata": {},
   "source": [
    "## Create Training and Testing data sets\n",
    "\n",
    "Now that we have loaded the tools we need, the first step in our journey is to be able to take our raw data and divide it into testing and training sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "693d83e0-8968-4ad8-9fea-56d881637b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#============================================================================\n",
    "#Determine what the training and testing percentages, of the data set, will be.\n",
    "#============================================================================\n",
    "training_portion = .80         # Use 80% of data for training, 20% for testing\n",
    "max_words        = 1000        # Max words in text input\n",
    "\n",
    "data             = pd.read_csv('testdata1.csv')\n",
    "\n",
    "#============================================================================\n",
    "# TRY:  uncomment the below print statement and print out first 5 rows of \n",
    "# generated claims so you can see what data looks like.\n",
    "#\n",
    "# print(data.head())\n",
    "#============================================================================\n",
    "\n",
    "train_size       = int(len(data) * training_portion)\n",
    "\n",
    "#============================================================================\n",
    "# FUNCTION:  train_test_split\n",
    "# This function splits the data into training and test sets.  \n",
    "# Inputs:   raw data and determined train_size.\n",
    "#============================================================================\n",
    "def train_test_split(data, train_size):\n",
    "    train        = data[:train_size]\n",
    "    test         = data[train_size:]\n",
    "    return train, test\n",
    "\n",
    "train_cat, test_cat   = train_test_split(data.iloc[:,1], train_size)  # label data is second column\n",
    "train_text, test_text = train_test_split(data.iloc[:,0], train_size)  # text data is first column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e0af16-5205-4479-84f8-8dce26e328b0",
   "metadata": {},
   "source": [
    "## Tokenize the Data sets\n",
    "\n",
    "After we have training and testing sets, we need to **tokenize the data**.  This means that we convert text documents into contextual vectors which contain numeric representations (index of where those words occur in a word dictionary) of the words in the documents.\n",
    "\n",
    "To see how Tokenization works, you can take a look at **02-TokenDemo.ipynb**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4d01f6a-4e86-4ce6-bc37-b31b7747a877",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize              = Tokenizer(num_words=max_words, char_level=False)\n",
    "tokenize.fit_on_texts(train_text) # fit tokenizer to our training text data\n",
    "\n",
    "#============================================================================\n",
    "#x_train and x_test are the vectorization of the text data (which is a claim)\n",
    "#============================================================================\n",
    "x_train               = tokenize.texts_to_matrix(train_text)\n",
    "x_test                = tokenize.texts_to_matrix(test_text)\n",
    "\n",
    "#============================================================================\n",
    "# TRY:  uncomment the below print statement and observe the rows in the \n",
    "# newly created matrix.\n",
    "#\n",
    "# print(x_train)\n",
    "# ===========================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1751d740-cc58-4d1a-8251-dcf5cc3ebc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#============================================================================\n",
    "# Use sklearn utility to convert label strings to numbered index\n",
    "#============================================================================\n",
    "encoder              = LabelEncoder()  \n",
    "encoder.fit(train_cat)\n",
    "y_train              = encoder.transform(train_cat)\n",
    "y_test               = encoder.transform(test_cat)\n",
    "\n",
    "#============================================================================\n",
    "# Note: for each row in the data, each entry represents the value of the label\n",
    "# Example:  [2 1 1 2 1 1 0 ...  which corresponds to starter, other,\n",
    "# other, starter, other, other, brakes ...\n",
    "#\n",
    "#============================================================================\n",
    "# TRY:  uncomment the below print statement.  What would you expect y_train\n",
    "# to look like?  \n",
    "#\n",
    "# print(y_train)\n",
    "#============================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcf16bf-c977-4900-8aa0-e45a719141e2",
   "metadata": {},
   "source": [
    "## One Hot Encoding\n",
    "\n",
    "We need to create labels (categories such as Brakes or Starter) for our test data, convert the labels to numbered index and then use one-hot encoding.\n",
    "\n",
    "**One hot encoding** allows the representation of categorical data to be more expressive. Many machine learning algorithms cannot work with categorical data directly. **The categories must be converted into numbers**. This is required for both input and output variables that are categorical.\n",
    "\n",
    "After we have converted the labels using one-hot encoding, we will be ready to build our main NLP model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26517820-122d-41e3-97f1-ef3b4a09289e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#============================================================================\n",
    "# Convert the labels to a one-hot representation\n",
    "#\n",
    "# One Hot Encoding replaces the column of labels whose (values are 0 or 1 or 2)\n",
    "# with 3 columns each representing 1 label value.  For example, the label \n",
    "# 'other' is replaced by the vector 0 1 0, the label 'starter' is replaced by\n",
    "# the vector 0 0 1, the label 'brakes' is replaced by the vector 1 0 0\n",
    "#============================================================================\n",
    "num_classes          = len(set(y_train))  # set() creates a unique set of objects\n",
    "y_train              = to_categorical(y_train, num_classes)  \n",
    "y_test               = to_categorical(y_test, num_classes)\n",
    "\n",
    "#============================================================================\n",
    "# TRY:  uncomment the below print statements in order to inspect the \n",
    "# dimenstions of our training and test data.\n",
    "# y_train may appear as y_train shape: (159, 3) which represents 159 rows, 3 cols\n",
    "#\n",
    "#print('x_train shape:', x_train.shape)\n",
    "#print('x_test shape:', x_test.shape)\n",
    "#print('y_train shape:', y_train.shape)\n",
    "#print('y_test shape:', y_test.shape)\n",
    "#============================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a6416d-6cd6-49d8-9aca-2379b0702a43",
   "metadata": {},
   "source": [
    "\n",
    "## Building the model\n",
    "\n",
    "Once the model is trained, we can test our model by entering a repair issue (e.g. the brakes feel soft when I press on them) and check if the model has correctly characterized the repair issue. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc05d7a6-7e54-4208-b743-0ffbb866dec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 0.9482 - accuracy: 0.6155 - val_loss: 0.5082 - val_accuracy: 0.9625\n",
      "Epoch 2/2\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4085 - accuracy: 0.9387 - val_loss: 0.2070 - val_accuracy: 0.9875\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2381 - accuracy: 0.9650\n"
     ]
    }
   ],
   "source": [
    "#============================================================================\n",
    "# Build model\n",
    "#============================================================================\n",
    "layers               = keras.layers\n",
    "models               = keras.models\n",
    "model                = models.Sequential()\n",
    "model.add(layers.Dense(512, input_shape=(max_words,), activation='relu'))  # Hidden layer with 512 nodes\n",
    "model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "#============================================================================\n",
    "# relu, softmax, categorical_crossentropy are telling the model how to do some \n",
    "# internal calculations.  Softmax is telling the model to calculate \n",
    "# probabilities for each category in each document.  If you only had yes, \n",
    "# or no you would use sigmoid instead of softmax.\n",
    "#============================================================================\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#============================================================================\n",
    "# VARIABLES\n",
    "# history    - normally used to plot learning curves.  \n",
    "# fit        - calculates the weights in the model. \n",
    "# batch_size - tells the internal calculations how many rows to process at 1 time\n",
    "# epochs     - num of times model calculations will pass through the entire data\n",
    "#============================================================================\n",
    "batch_size          = 32\n",
    "epochs              = 2\n",
    "history = model.fit(x_train, y_train,      \n",
    "                    batch_size=batch_size,  \n",
    "                    epochs=epochs,         \n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)\n",
    "\n",
    "#============================================================================\n",
    "# evaluate func compares the model predictions with the actual known test values\n",
    "#============================================================================\n",
    "score = model.evaluate(x_test, y_test,       \n",
    "                       batch_size=batch_size, verbose=1)\n",
    "\n",
    "#============================================================================\n",
    "# TRY:  uncomment the below print statements to see the test loss and accuracy\n",
    "# of our model\n",
    "#\n",
    "#print('Test loss:', score[0])\n",
    "#print('Test accuracy:', score[1])\n",
    "#============================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cecb48-13d4-4387-ac8f-37bfc797c8f4",
   "metadata": {},
   "source": [
    "\n",
    "## Let's test our model!\n",
    "\n",
    "Now that we have a model, we would like to generate a prediction (e.g. categorize the repair issue as:  Brakes, Starter or Other)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "157b9c35-7cf9-4302-b489-64fb2df23e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#============================================================================\n",
    "# In this section, we will need to create a 'predict' function that will pass \n",
    "# a 'repair string' to the model.  The return string will be a category of \n",
    "# Brake, Starter or Other\n",
    "#============================================================================\n",
    "text_labels = encoder.classes_   #ndarray of output values (labels or classes)  e.g. other, brakes, starter\n",
    "\n",
    "#============================================================================\n",
    "# Examine first 10 test samples of 445\n",
    "#============================================================================\n",
    "for i in range(len(test_cat)):\n",
    "    temp = x_test[i]\n",
    "    prediction = model.predict(np.array([x_test[i]]))\n",
    "    predicted_label = text_labels[np.argmax(prediction)]  #predicted class\n",
    "    #print(test_text.iloc[i][:50], \"...\")                # 50 char sample of text\n",
    "    #print('Actual label:' + test_cat.iloc[i])\n",
    "    #print(\"Predicted label: \" + predicted_label + \"\\n\")\n",
    "\n",
    "#============================================================================\n",
    "# FUNCTION:  predict\n",
    "# This function takes a string input (e.g. repair issue), tokenizes the input\n",
    "# and then passes the input to the model.  The 'predicted_label' maps the\n",
    "# index of the prediction to the test labels array (e.g. Brakes, Starter or Other)\n",
    "# The single_predicted_label returns one of the test labels array (e.g. Brakes)\n",
    "#============================================================================\n",
    "def predict(single_test_text):\n",
    "    text_as_series = pd.Series(single_test_text) #perform data conversion\n",
    "    single_x_test = tokenize.texts_to_matrix(text_as_series)\n",
    "    single_prediction = model.predict(np.array([single_x_test]))\n",
    "    single_predicted_label = text_labels[np.argmax(single_prediction)]  #maps index of the prediction to the test labels array e.g. brakes\n",
    "    return (single_predicted_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "70e603b3-f4e2-428f-857b-8829d4ba270f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==========================================================================\n",
    "# TRY: uncomment the below to test the predict function.  We will test 3\n",
    "# repair cases so that we can see if the model can properly categorize\n",
    "# a Brake, a Starter and an Other repair text\n",
    "\n",
    "#single_test_text = 'press brake pedal and car wont stop'\n",
    "#single_test_text = 'turn key over and hear a clicking sound' \n",
    "#single_test_text = 'there is fluid leaking from the engine' \n",
    "\n",
    "#prediction = predict(single_test_text) \n",
    "#print(prediction)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
