{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94a04113-590f-4941-a003-ac8d1a792faa",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "Tokenization is the process of **converting text documentation into contextual vectors which contain numeric representations** (index of where those words occur in a work dictionary) of the words in the documents. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ecc36b-ecf9-4e63-92e2-d0f52df988b1",
   "metadata": {},
   "source": [
    "## Environment initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db88b9cc-d60e-42c5-aaf9-a4dac31b517b",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Of course, we'll need to import various packages. They are either built in the notebook image you are running, or have been installed in the previous step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad20a104-27d8-40d4-827e-28dffdc44841",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4b12cf0ddd13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhashing_trick\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_to_word_sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import hashing_trick, text_to_word_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45372c50-a74b-42b5-bb23-9ce81188f8fa",
   "metadata": {},
   "source": [
    "\n",
    "In the following example, we define 5 'text' documents.  Each word in the document is gong to be placed into a dictionary and will be given a unique index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e69d335e-e18a-41af-aeda-5448a6038281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word counts:  \n",
      "OrderedDict([('well', 1), ('done', 1), ('good', 1), ('work', 2), ('great', 1), ('effort', 1), ('nice', 1), ('excellent', 1)])\n",
      "Document count:  \n",
      "5\n",
      "Word index:  \n",
      "{'work': 1, 'well': 2, 'done': 3, 'good': 4, 'great': 5, 'effort': 6, 'nice': 7, 'excellent': 8}\n",
      "Word docs:  \n",
      "defaultdict(<class 'int'>, {'done': 1, 'well': 1, 'work': 2, 'good': 1, 'effort': 1, 'great': 1, 'nice': 1, 'excellent': 1})\n",
      "Encoded docs matrix:  \n",
      "[[0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#============================================================================\n",
    "# Define 5 'text' documents\n",
    "#============================================================================\n",
    "docs = ['Well done!',\n",
    "\t\t'Good work',\n",
    "\t\t'Great effort',\n",
    "\t\t'nice work',\n",
    "\t\t'Excellent!']\n",
    "\n",
    "#============================================================================\n",
    "# Create the tokenizer\n",
    "#============================================================================\n",
    "t = Tokenizer()\n",
    "\n",
    "#============================================================================\n",
    "# Fit the tokenizer on the documents.  Creates a word index and other info\n",
    "#============================================================================\n",
    "t.fit_on_texts(docs)  \n",
    "\n",
    "#============================================================================\n",
    "# Summarize what was learned. \n",
    "\n",
    "#============================================================================\n",
    "# Dict of words and their count. In this example there are 8 unique words\n",
    "#============================================================================\n",
    "print(\"Word counts:  \\n{}\".format(t.word_counts)) \n",
    "\n",
    "#============================================================================\n",
    "# Number of docs in the fit\n",
    "#============================================================================\n",
    "print(\"Document count:  \\n{}\".format(t.document_count)) \n",
    "\n",
    "#============================================================================\n",
    "# Dictionary of words and their indexes. Index starts at 1 since 0 is reserved.\n",
    "#============================================================================\n",
    "print(\"Dictionary or Word index:  \\n{}\".format(t.word_index)) \n",
    "\n",
    "#============================================================================\n",
    "# Dictionary of words and how many docs each appeared in\n",
    "#============================================================================\n",
    "print(\"Word docs:  \\n{}\".format(t.word_docs)) \n",
    "\n",
    "#============================================================================\n",
    "# Integer encode documents\n",
    "# Matrix of words of each doc. Uses t.word_index for each doc.\n",
    "#============================================================================\n",
    "encoded_docs = t.texts_to_matrix(docs) \n",
    "\n",
    "#============================================================================\n",
    "#this is the 'contextual vector form' of the original textual document 'docs'\n",
    "#============================================================================\n",
    "print(\"Encoded docs matrix:  \\n{}\".format(encoded_docs)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54540203-bea5-445e-a848-599bc5906a64",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Let's take a closer look at what happened in the above code!\n",
    "\n",
    "For example, the first row in the encoded matrix is:\n",
    "[0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
    "\n",
    "Recall that our Word index is the following: \n",
    "{'work': 1, 'well': 2, 'done': 3, 'good': 4, 'great': 5, 'effort': 6, 'nice': 7, 'excellent': 8}\n",
    "\n",
    "The first row in the encoded matrix, contains two words, because there are 2 non-zero entries in the vector.  One of the words has an index of 2, and the other word has and index of 3 in the Word index.  Therefore the first document contains the 2 words 'well' and 'done'.\n",
    "\n",
    "**Note:  our matrix contains as many columns as there are unique words in the dictionary.**  The non-zero values in each row represent the word indexes of the all words in the document.\n",
    "\n",
    "Now that we understand Tokenization let's go back to our claims data and vectorize it.  Go back to the notebook **01-Create-Claims-Classification.ipynb**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
